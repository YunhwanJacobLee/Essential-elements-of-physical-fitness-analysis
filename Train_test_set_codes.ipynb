{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zYYQis3opGm7"
      ],
      "authorship_tag": "ABX9TyO0ZvJkB1jxDxTUWhPmU8Ga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YunhwanJacobLee/Essential-elements-of-physical-fitness-analysis/blob/main/Train_test_set_codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADRcPtw9yxzv"
      },
      "source": [
        "#  Sports Classification using Machine Learning Methods"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# data splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data stanard\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# data modeling\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "#Elastic Model#\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "#ANN\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "mn-Mk7J8aZb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Train, Test"
      ],
      "metadata": {
        "id": "pYkQc3324n10"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTzhzTy7yoJX",
        "outputId": "6321c03b-0e24-4cfa-bccf-691d5a409406"
      },
      "source": [
        "####################\n",
        "X = teen_running_soccer\n",
        "y = teen_runsoc_target\n",
        "####################\n",
        "\n",
        "print(y.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    938\n",
            "1    714\n",
            "2    368\n",
            "3    228\n",
            "4    179\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGwDvpnxy4IH",
        "outputId": "ca9f91f6-a63a-4274-f890-8e7828e95fbe"
      },
      "source": [
        "\"\"\" Class 간의 비율 유지: stratify \"\"\"\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, stratify = y, random_state = 42)\n",
        "\n",
        "print(\"*** Train Dataset ***\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\n*** Test Dataset ***\")\n",
        "print('\\n',y_test.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Train Dataset ***\n",
            "0    750\n",
            "1    571\n",
            "2    294\n",
            "3    183\n",
            "4    143\n",
            "dtype: int64\n",
            "\n",
            "*** Test Dataset ***\n",
            "\n",
            " 0    188\n",
            "1    143\n",
            "2     74\n",
            "3     45\n",
            "4     36\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "train_scaled = scaler.transform(X_train)\n",
        "test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "3YDIVE8Y7gBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Over Sampling: Synthetic Minority Oversampling Technique (SMOTE) \"\"\"\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE  #SVMSMOTE !!\n",
        "\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_sm, y_train_sm = sm.fit_resample(train_scaled, y_train)\n",
        "\n",
        "print('Resampled dataset shape {}'.format(Counter(y_train_sm)))"
      ],
      "metadata": {
        "id": "HwN1p7Xf2rx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dbcc3d6-7a6d-4cb7-9740-dfd771ef92da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled dataset shape Counter({0: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) fit train, val set"
      ],
      "metadata": {
        "id": "0beVbAK95H8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "num_epoch = 1000\n",
        "LR_coarse_hyperparameter_list = []\n",
        "\n",
        "\"\"\" Logistic Regressioin \"\"\"\n",
        "for epoch in range(num_epoch):\n",
        "  C = np.random.uniform(0.01, 100)\n",
        "  max_iter = int(np.random.uniform(10, 1000))\n",
        "\n",
        "  LR_model = LogisticRegression(C = C, max_iter=max_iter,\n",
        "                                n_jobs=-1,\n",
        "                                random_state=42)\n",
        "\n",
        "  score = cross_val_score(LR_model, X_train_sm, y_train_sm, cv=5)\n",
        "  print(\"epoch = {0}, C = {1}, max_iter = {2}, score = {3:.5f}\" \\\n",
        "        .format(epoch, C, max_iter, score.mean()))\n",
        "\n",
        "  hyperparameter = {'C': C,\n",
        "                    'max_iter': max_iter,\n",
        "                    'score': score.mean()}\n",
        "\n",
        "  LR_coarse_hyperparameter_list.append(hyperparameter)\n",
        "\n",
        "print(\"\\n***** Logistic Regression *****\")\n",
        "LR_coarse_hyperparameter_list = pd.DataFrame.from_dict(LR_coarse_hyperparameter_list)\n",
        "LR_coarse_hyperparameter_list = LR_coarse_hyperparameter_list.sort_values(\"score\", ascending=False)\n",
        "LR_coarse_hyperparameter_list.head(10)"
      ],
      "metadata": {
        "id": "lVjjulyeF_Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 1000\n",
        "SVM_coarse_hyperparameter_list = []\n",
        "\n",
        "\"\"\" SVM \"\"\"\n",
        "for epoch in range(num_epoch):\n",
        "  C = np.random.uniform(0.01, 100)\n",
        "  gamma = np.random.uniform(0.01, 100)\n",
        "\n",
        "  SVM_model = SVC(C = C, gamma=gamma, random_state=42)\n",
        "\n",
        "  score = cross_val_score(SVM_model, X_train_sm, y_train_sm, cv=5)\n",
        "  print(\"epoch = {0}, C = {1}, gamma = {2}, score = {3:.5f}\" \\\n",
        "        .format(epoch, C, gamma, score.mean()))\n",
        "\n",
        "  hyperparameter = {'C': C,\n",
        "                    'gamma': gamma,\n",
        "                    'score': score.mean()}\n",
        "\n",
        "  SVM_coarse_hyperparameter_list.append(hyperparameter)\n",
        "\n",
        "print(\"\\n***** SVM *****\")\n",
        "SVM_coarse_hyperparameter_list = pd.DataFrame.from_dict(SVM_coarse_hyperparameter_list)\n",
        "SVM_coarse_hyperparameter_list = SVM_coarse_hyperparameter_list.sort_values(\"score\", ascending=False)\n",
        "SVM_coarse_hyperparameter_list.head(10)"
      ],
      "metadata": {
        "id": "qIcoRQB2Oqst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 1000\n",
        "RF_coarse_hyperparameter_list = []\n",
        "\n",
        "\"\"\" Random Forest \"\"\"\n",
        "for epoch in range(num_epoch):\n",
        "  n_estimators =  int(np.random.uniform(20, 150))\n",
        "  max_depth = int(np.random.uniform(1, 12))\n",
        "  max_features = np.random.uniform(0.1, 1.0)\n",
        "  min_samples_split = int(np.random.uniform(1, 40))\n",
        "  min_samples_leaf = int(np.random.uniform(1, 20))\n",
        "\n",
        "  RF_model = RandomForestClassifier(n_estimators=n_estimators,\n",
        "                                    max_depth=max_depth,\n",
        "                                    max_features=max_features,\n",
        "                                    min_samples_split=min_samples_split,\n",
        "                                    min_samples_leaf=min_samples_leaf,\n",
        "                                    n_jobs=-1,\n",
        "                                    random_state=42)\n",
        "\n",
        "  score = cross_val_score(RF_model, X_train_sm, y_train_sm, cv=5)\n",
        "  print(\"epoch = {0}, n_estimators = {1}, max_depth = {2}, max_features = {3}, min_samples_split = {4}, min_samples_leaf = {5}, score = {6:.5f}\" \\\n",
        "        .format(epoch, n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf, score.mean()))\n",
        "\n",
        "  hyperparameter = {'n_estimators': n_estimators,\n",
        "                    'max_depth': max_depth,\n",
        "                    'max_features': max_features,\n",
        "                    'min_samples_split': min_samples_split,\n",
        "                    'min_samples_leaf': min_samples_leaf,\n",
        "                    'score': score.mean()}\n",
        "\n",
        "  RF_coarse_hyperparameter_list.append(hyperparameter)\n",
        "\n",
        "print(\"\\n***** Random Forest *****\")\n",
        "RF_coarse_hyperparameter_list = pd.DataFrame.from_dict(RF_coarse_hyperparameter_list)\n",
        "RF_coarse_hyperparameter_list = RF_coarse_hyperparameter_list.sort_values(\"score\", ascending=False)\n",
        "RF_coarse_hyperparameter_list.head(10)"
      ],
      "metadata": {
        "id": "7PoxKbOSOrE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBl1R5_by8zf"
      },
      "source": [
        "num_epoch = 1000\n",
        "XGB_coarse_hyperparameter_list = []\n",
        "\n",
        "\"\"\" XGBoost \"\"\"\n",
        "for epoch in range(num_epoch):\n",
        "  n_estimators =  int(np.random.uniform(20, 150))\n",
        "  max_depth = int(np.random.uniform(1, 12))\n",
        "  max_features = np.random.uniform(0.1, 1.0)\n",
        "\n",
        "  XGB_model = XGBClassifier(n_estimators=n_estimators,\n",
        "                                    max_depth=max_depth,\n",
        "                                    max_features=max_features,\n",
        "                                    n_jobs=-1,\n",
        "                                    random_state=42)\n",
        "\n",
        "  score = cross_val_score(XGB_model, X_train_sm, y_train_sm, cv=5)\n",
        "  print(\"epoch = {0}, n_estimators = {1}, max_depth = {2}, max_features = {3}, score = {4:.5f}\" \\\n",
        "        .format(epoch, n_estimators, max_depth, max_features, score.mean()))\n",
        "\n",
        "  hyperparameter = {'n_estimators': n_estimators,\n",
        "                    'max_depth': max_depth,\n",
        "                    'max_features': max_features,\n",
        "                    'score': score.mean()}\n",
        "\n",
        "  XGB_coarse_hyperparameter_list.append(hyperparameter)\n",
        "\n",
        "print(\"\\n***** XGBoost *****\")\n",
        "XGB_coarse_hyperparameter_list = pd.DataFrame.from_dict(XGB_coarse_hyperparameter_list)\n",
        "XGB_coarse_hyperparameter_list = XGB_coarse_hyperparameter_list.sort_values(\"score\", ascending=False)\n",
        "XGB_coarse_hyperparameter_list.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "num_epoch = 1000\n",
        "EL_coarse_hyperparameter_list = []\n",
        "\n",
        "\n",
        "\"\"\" Logistic Regressioin \"\"\"\n",
        "for epoch in range(num_epoch):\n",
        "  alpha = np.random.uniform(0, 100)\n",
        "  l1_ratio = int(np.random.uniform(0.1, 1))\n",
        "\n",
        "  EN_model = ElasticNet(alpha = alpha, l1_ratio=l1_ratio, random_state=42)\n",
        "\n",
        "  score = cross_val_score(EN_model, X_train_sm, y_train_sm, cv=5)\n",
        "  print(\"epoch = {0}, alpha = {1}, l1_ratio = {2}, score = {3:.5f}\" \\\n",
        "        .format(epoch, alpha, l1_ratio, score.mean()))\n",
        "\n",
        "  hyperparameter = {'alpha': alpha,\n",
        "                    'l1_ratio': l1_ratio,\n",
        "                    'score': score.mean()}\n",
        "\n",
        "  EL_coarse_hyperparameter_list.append(hyperparameter)\n",
        "\n",
        "print(\"\\n***** Logistic Regression *****\")\n",
        "EL_coarse_hyperparameter_list = pd.DataFrame.from_dict(EL_coarse_hyperparameter_list)\n",
        "EL_coarse_hyperparameter_list = EL_coarse_hyperparameter_list.sort_values(\"score\", ascending=False)\n",
        "EL_coarse_hyperparameter_list.head(10)"
      ],
      "metadata": {
        "id": "z9owbKdESNEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "cvscores = []\n",
        "conf = []\n",
        "f1 = []\n",
        "auc = []\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for train, test in kfold.split(X_train_sm, y_train_sm):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=10, activation='relu', input_shape=(15,)))\n",
        "    model.add(Dense(units=5, activation='sigmoid'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train_sm, y_train_sm, epochs=200, batch_size=32)\n",
        "    scores = model.evaluate(X_test, y_test)\n",
        "    print(f'{model.metrics_names[1]}: {scores[1]*100:.2f}%')\n",
        "    cvscores.append(scores[1] * 100)"
      ],
      "metadata": {
        "id": "ssjhnBmOS_zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Test sets"
      ],
      "metadata": {
        "id": "-C3gVxOxPfxE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jBZgcBUy_FP"
      },
      "source": [
        "## (1) Adult\n",
        "LR = LogisticRegression(C = 36.380497, max_iter = 11, multi_class='ovr')\n",
        "SVM = SVC(C = 39.689206, gamma = 0.551907, probability=False)\n",
        "RF = RandomForestClassifier(n_estimators = 124, max_depth = 11, max_features = 0.356285, min_samples_split = 3, min_samples_leaf = 3)\n",
        "xgb = XGBClassifier(n_estimators = 147, max_depth = 8, max_features = 0.353969)\n",
        "\n",
        "## (2) Model Training\n",
        "LR_model = LR.fit(X_train_sm, y_train_sm)\n",
        "SVM_model = SVM.fit(X_train_sm, y_train_sm)\n",
        "RF_model = RF.fit(X_train_sm, y_train_sm)\n",
        "XGB_model = xgb.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "\"\"\" save model \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## (3) Model Prediction\n",
        "LR_predict = LR_model.predict(test_scaled)\n",
        "SVM_predict = SVM_model.predict(test_scaled)\n",
        "RF_predict = RF_model.predict(test_scaled)\n",
        "XGB_predict = XGB_model.predict(test_scaled)\n",
        "\n",
        "## (4) Model Evaluation\n",
        "LR_conf_matrix = confusion_matrix(y_test, LR_predict)\n",
        "LR_acc_score = accuracy_score(y_test, LR_predict)\n",
        "print(\"\\n***** Logistc Regression *****\")\n",
        "print(\"confussion matrix\")\n",
        "print(LR_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Logistic Regression:\",LR_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,LR_predict))\n",
        "\n",
        "print(\"\\n***** SVM *****\")\n",
        "SVM_conf_matrix = confusion_matrix(y_test, SVM_predict)\n",
        "SVM_acc_score = accuracy_score(y_test, SVM_predict)\n",
        "print(\"confussion matrix\")\n",
        "print(SVM_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Support Vector Classifier:\",SVM_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,SVM_predict))\n",
        "\n",
        "\n",
        "print(\"\\n***** Decision Tree *****\")\n",
        "DT_conf_matrix = confusion_matrix(y_test, DT_predict)\n",
        "DT_acc_score = accuracy_score(y_test, DT_predict)\n",
        "print(\"confussion matrix\")\n",
        "print(DT_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Decision Tree:\",DT_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,DT_predict))\n",
        "\n",
        "\n",
        "print(\"\\n***** Random Forest *****\")\n",
        "RF_conf_matrix = confusion_matrix(y_test, RF_predict)\n",
        "RF_acc_score = accuracy_score(y_test, RF_predict)\n",
        "print(\"confussion matrix\")\n",
        "print(RF_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Random Forest:\",RF_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,RF_predict))\n",
        "\n",
        "print(\"\\n***** XGB *****\")\n",
        "XGB_conf_matrix = confusion_matrix(y_test, XGB_predict)\n",
        "XGB_acc_score = accuracy_score(y_test, XGB_predict)\n",
        "print(\"confussion matrix\")\n",
        "print(XGB_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of XGBoost:\",XGB_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,XGB_predict))\n",
        "print(\"XGB_auc_score\")"
      ],
      "metadata": {
        "id": "Yoa1mPJlcyXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## (3) Model Prediction\n",
        "LR_predict = LR_model.predict(test_scaled)\n",
        "SVM_predict = SVM_model.predict(test_scaled)\n",
        "RF_predict = RF_model.predict(test_scaled)\n",
        "XGB_predict = XGB_model.predict(test_scaled)\n",
        "\n",
        "## (4) Model Evaluation\n",
        "LR_conf_matrix = confusion_matrix(y_test, LR_predict)\n",
        "LR_acc_score = accuracy_score(y_test, LR_predict)\n",
        "LR_auc_score = roc_auc_score(y_test, LR_predict)\n",
        "LR_tn, LR_fp, LR_fn, LR_tp = confusion_matrix(y_test, LR_predict).ravel()\n",
        "LR_NPV = LR_tn/(LR_tn + LR_fn)\n",
        "LR_PPV = LR_tp/(LR_tp + LR_fp)\n",
        "LR_specificity = LR_tn/(LR_tn + LR_fp)\n",
        "LR_prob = LR_model.predict_proba(test_scaled)[:, 1]\n",
        "LR_fpr, LR_tpr, thresholds = roc_curve(y_test, LR_prob)\n",
        "plt.plot(LR_fpr, LR_tpr, color='red', label='ROC')\n",
        "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend()\n",
        "plt.savefig(roc_curve_data_dir +'Middle_LR_ROC_curve_swimming vs badminton.png', dpi=300)\n",
        "plt.show()\n",
        "print(\"\\n***** Logistc Regression *****\")\n",
        "print(\"confussion matrix\")\n",
        "print(LR_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Logistic Regression:\",LR_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,LR_predict))\n",
        "print(\"LR_auc_score\")\n",
        "print(LR_auc_score)\n",
        "print(\"\\n\")\n",
        "print(\"NPV, PPV, Specificity\")\n",
        "print(LR_NPV)\n",
        "print(LR_PPV)\n",
        "print(LR_specificity)\n",
        "\n",
        "print(\"\\n***** SVM *****\")\n",
        "SVM_conf_matrix = confusion_matrix(y_test, SVM_predict)\n",
        "SVM_acc_score = accuracy_score(y_test, SVM_predict)\n",
        "SVM_auc_score = roc_auc_score(y_test, SVM_predict)\n",
        "SVM_tn, SVM_fp, SVM_fn, SVM_tp = confusion_matrix(y_test, SVM_predict).ravel()\n",
        "SVM_NPV = SVM_tn/(SVM_tn + SVM_fn)\n",
        "SVM_PPV = SVM_tp/(SVM_tp + SVM_fp)\n",
        "SVM_specificity = SVM_tn/(SVM_tn + SVM_fp)\n",
        "SVM_prob = SVM_model.predict_proba(test_scaled)[:, 1]\n",
        "SVM_fpr, SVM_tpr, thresholds = roc_curve(y_test, SVM_prob)\n",
        "plt.plot(SVM_fpr, SVM_tpr, color='red', label='ROC')\n",
        "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend()\n",
        "plt.savefig(roc_curve_data_dir +'Middle_SVM_ROC_curve_swimming vs badminton.png', dpi=300)\n",
        "plt.show()\n",
        "print(\"confussion matrix\")\n",
        "print(SVM_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Support Vector Classifier:\",SVM_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,SVM_predict))\n",
        "print(\"SVM_auc_score\")\n",
        "print(SVM_auc_score)\n",
        "print(\"\\n\")\n",
        "print(\"NPV, PPV, Specificity\")\n",
        "print(SVM_NPV)\n",
        "print(SVM_PPV)\n",
        "print(SVM_specificity)\n",
        "\n",
        "print(\"\\n***** Random Forest *****\")\n",
        "RF_conf_matrix = confusion_matrix(y_test, RF_predict)\n",
        "RF_acc_score = accuracy_score(y_test, RF_predict)\n",
        "RF_auc_score = roc_auc_score(y_test, RF_predict)\n",
        "RF_tn, RF_fp, RF_fn, RF_tp = confusion_matrix(y_test, RF_predict).ravel()\n",
        "RF_NPV = RF_tn/(RF_tn + RF_fn)\n",
        "RF_PPV = RF_tp/(RF_tp + RF_fp)\n",
        "RF_specificity = RF_tn/(RF_tn + RF_fp)\n",
        "RF_prob = RF_model.predict_proba(test_scaled)[:, 1]\n",
        "RF_fpr, RF_tpr, thresholds = roc_curve(y_test, RF_prob)\n",
        "plt.plot(RF_fpr, RF_tpr, color='red', label='ROC')\n",
        "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend()\n",
        "plt.savefig(roc_curve_data_dir +'Middle_RF_ROC_curve_swimming vs badminton.png', dpi=300)\n",
        "plt.show()\n",
        "print(\"confussion matrix\")\n",
        "print(RF_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Random Forest:\",RF_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,RF_predict))\n",
        "print(\"RF_auc_score\")\n",
        "print(RF_auc_score)\n",
        "print(\"\\n\")\n",
        "print(\"NPV, PPV, Specificity\")\n",
        "print(RF_NPV)\n",
        "print(RF_PPV)\n",
        "print(RF_specificity)\n",
        "\n",
        "print(\"\\n***** XGB *****\")\n",
        "XGB_conf_matrix = confusion_matrix(y_test, XGB_predict)\n",
        "XGB_acc_score = accuracy_score(y_test, XGB_predict)\n",
        "XGB_auc_score = roc_auc_score(y_test, XGB_predict)\n",
        "XGB_tn, XGB_fp, XGB_fn, XGB_tp = confusion_matrix(y_test, XGB_predict).ravel()\n",
        "XGB_NPV = XGB_tn/(XGB_tn + XGB_fn)\n",
        "XGB_PPV = XGB_tp/(XGB_tp + XGB_fp)\n",
        "XGB_specificity = XGB_tn/(XGB_tn + XGB_fp)\n",
        "XGB_prob = XGB_model.predict_proba(test_scaled)[:, 1]\n",
        "XGB_fpr, XGB_tpr, thresholds = roc_curve(y_test, XGB_prob)\n",
        "plt.plot(XGB_fpr, XGB_tpr, color='red', label='ROC')\n",
        "plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend()\n",
        "plt.savefig(roc_curve_data_dir +'Middle_XGB_ROC_curve_swimming vs badminton.png', dpi=300)\n",
        "plt.show()\n",
        "print(\"confussion matrix\")\n",
        "print(XGB_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of XGBoost:\",XGB_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,XGB_predict))\n",
        "print(\"XGB_auc_score\")\n",
        "print(XGB_auc_score)\n",
        "print(\"\\n\")\n",
        "print(\"NPV, PPV, Specificity\")\n",
        "print(XGB_NPV)\n",
        "print(XGB_PPV)\n",
        "print(XGB_specificity)\n"
      ],
      "metadata": {
        "id": "vxPI_Mt2G3Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### elastic Net ###\n",
        "\n",
        "elastic_net_classifier = LogisticRegressionCV(cv=5, penalty='elasticnet', l1_ratios=[0.1, 0.5, 0.9], solver='saga')\n",
        "EL_model = elastic_net_classifier.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "EL_predict = EL_model.predict(X_test)\n",
        "\n",
        "EL_conf_matrix = confusion_matrix(y_test, EL_predict)\n",
        "EL_acc_score = accuracy_score(y_test, EL_predict)\n",
        "EL_auc_score = roc_auc_score(y_test, EL_predict)\n",
        "EL_tn, EL_fp, EL_fn, EL_tp = confusion_matrix(y_test, EL_predict).ravel()\n",
        "EL_NPV = EL_tn/(EL_tn + EL_fn)\n",
        "EL_PPV = EL_tp/(EL_tp + EL_fp)\n",
        "EL_specificity = EL_tn/(EL_tn + EL_fp)\n",
        "EL_sensitivity = EL_tp/(EL_tp + EL_fn)\n",
        "EL_prob = EL_model.predict_proba(X_test)[:, 1]\n",
        "EL_fpr, EL_tpr, thresholds = roc_curve(y_test, EL_prob)\n",
        "print(\"\\n***** Logistc Regression *****\")\n",
        "print(\"confussion matrix\")\n",
        "print(EL_conf_matrix)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy of Logistic Regression:\",EL_acc_score*100,'\\n')\n",
        "print(classification_report(y_test,EL_predict))\n",
        "print(\"EL_auc_score\")\n",
        "print(EL_auc_score)\n",
        "print(\"\\n\")\n",
        "print(\"NPV, PPV, Specificity\")\n",
        "print(EL_NPV)\n",
        "print(EL_PPV)\n",
        "print(EL_specificity)\n",
        "print(EL_sensitivity)"
      ],
      "metadata": {
        "id": "dCDZcOo8R3qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ANN ###\n",
        "\n",
        "import numpy\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "cvscores = []\n",
        "conf = []\n",
        "f1 = []\n",
        "auc = []\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for train, test in kfold.split(X_train_sm, y_train_sm):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=10, activation='relu', input_shape=(15,)))\n",
        "    model.add(Dense(units=5, activation='sigmoid'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train_sm, y_train_sm, epochs=200, batch_size=32)\n",
        "    scores = model.evaluate(X_test, y_test)\n",
        "    print(f'{model.metrics_names[1]}: {scores[1]*100:.2f}%')\n",
        "    cvscores.append(scores[1] * 100)\n",
        "\n",
        "    ANN_predict = model.predict(X_test)\n",
        "    ANN_predict = (ANN_predict>0.5)\n",
        "\n",
        "    ANN_acc_score = accuracy_score(y_test, ANN_predict)\n",
        "    ANN_auc_score = roc_auc_score(y_test, ANN_predict)\n",
        "    ANN_f1_score = f1_score(y_test, ANN_predict)\n",
        "    ANN_tn, ANN_fp, ANN_fn, ANN_tp = confusion_matrix(y_test, ANN_predict).ravel()\n",
        "    ANN_NPV = ANN_tn/(ANN_tn + ANN_fn)\n",
        "    ANN_PPV = ANN_tp/(ANN_tp + ANN_fp)\n",
        "    ANN_specificity = ANN_tn/(ANN_tn + ANN_fp)\n",
        "    ANN_sensitivity = ANN_tp/(ANN_tp + ANN_fn)\n",
        "    conf.append(ANN_conf_matrix)\n",
        "    auc.append(ANN_auc_score)\n",
        "    f1.append(ANN_f1_score)\n",
        "    print(\"\\n***** ANN *****\")\n",
        "    print(\"confussion matrix\")\n",
        "    print(ANN_conf_matrix)\n",
        "    print(\"\\n\")\n",
        "    print(\"Accuracy of ANN:\",ANN_acc_score*100,'\\n')\n",
        "    print(\"ANN_auc_score\")\n",
        "    print(ANN_auc_score)\n",
        "    print(\"\\n\")\n",
        "    print(\"NPV, PPV, Specificity\")\n",
        "\n",
        "\n",
        "print(numpy.mean(cvscores), numpy.std(cvscores))\n",
        "\n",
        "avg_cm = np.mean(conf, axis=0)\n",
        "print(avg_cm)\n",
        "\n",
        "TN = avg_cm[0, 0]\n",
        "FP = avg_cm[0, 1]\n",
        "FN = avg_cm[1, 0]\n",
        "TP = avg_cm[1, 1]\n",
        "\n",
        "sensitivity = TP / (TP + FN)\n",
        "specificity = TN / (TN + FP)\n",
        "\n",
        "auc = np.mean(auc)\n",
        "f1 = np.mean(f1)\n",
        "print(\"accuracy\", numpy.mean(cvscores), numpy.std(cvscores))\n",
        "print(\"AUC\", auc)\n",
        "print(\"F1\", f1)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Sensitivity:\", sensitivity)"
      ],
      "metadata": {
        "id": "xsfEKzHSTB8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) PCA, feature importance, SHAP value"
      ],
      "metadata": {
        "id": "3HZ38KiTWNd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### PCA ###\n",
        "\n",
        "from sklearn import decomposition\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "X = scale(teen_running_soccer)\n",
        "x = StandardScaler().fit_transform(X)\n",
        "\n",
        "# apply PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "Principal_components=pca.fit_transform(teen_running_soccer)\n",
        "\n",
        "sports_label = ['Body Fat', 'Body Weight', 'BMI','Grip Strength (L)', 'Grip Strength (R)','Grip Strength (avg)','Back Muscle Strength', 'Push-up', 'Sit-up', 'Standing Long Jump',\n",
        "                 'Sargent Jump', 'Side-step', 'Backward Flexion',\n",
        "                 'Sit & Reach', 'Eye-Hand Coordination']\n",
        "\n",
        "loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=sports_label)"
      ],
      "metadata": {
        "id": "go1wAtR6XMMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### XGBoost feature importance ###\n",
        "\n",
        "sports_label = ['BF', 'BW', 'BMI','GS (L)', 'GS (R)','GS (avg)','BMS', 'PU', 'SU', 'SLJ',\n",
        "                 'SJ', 'SS', 'BF', 'SR', 'EHC']\n",
        "\n",
        "importances = XGB_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "names = [sports_label[i] for i in indices]\n",
        "width = 0.8\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X_test.shape[1]):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "\n",
        "# Plot the impurity-based feature importances of the forest\n",
        "plt.figure(figsize=(11, 10))\n",
        "plt.barh(range(X_test.shape[1]), importances[indices],\n",
        "        color=\"tab:blue\", align=\"center\", height=0.85)\n",
        "plt.yticks(range(X_test.shape[1]), names)\n",
        "plt.rc('xtick', labelsize=20)\n",
        "plt.rc('ytick', labelsize=30)\n",
        "plt.ylim([X_test.shape[1], -1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cYNozAY9Wsme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SHAP value\n",
        "\n",
        "import shap\n",
        "\n",
        "explainer = shap.Explainer(XGB_model)\n",
        "expected_value = explainer.expected_value\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "shap.plots.beeswarm(shap_values, max_display=20, show=False)"
      ],
      "metadata": {
        "id": "_pGN_UBHYo1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}